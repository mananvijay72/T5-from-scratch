{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1febcbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autograd_tensor.py\n",
    "# Minimal numpy-based autograd engine with broadcasting and batch support.\n",
    "# Ops: +, -, neg, *, **, sqrt, @, /, T, reshape, exp, log, mean, gather, sum\n",
    "# sum and gather support axis/dim. T swaps the last two dims for ND tensors.\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Union, Iterable\n",
    "\n",
    "ArrayLike = Union[np.ndarray, float, int]\n",
    "\n",
    "def _as_array(x: ArrayLike, dtype=np.float32) -> np.ndarray:\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(dtype, copy=False)\n",
    "    return np.array(x, dtype=dtype)\n",
    "\n",
    "def _normalize_axes(axis, ndim: int) -> Optional[Tuple[int, ...]]:\n",
    "    if axis is None:\n",
    "        return None\n",
    "    if isinstance(axis, (list, tuple)):\n",
    "        axes = tuple((a + ndim) if a < 0 else a for a in axis)\n",
    "        return tuple(sorted(axes))\n",
    "    a = axis + ndim if axis < 0 else axis\n",
    "    return (a,)\n",
    "\n",
    "def _sum_to_shape(x: np.ndarray, shape: Tuple[int, ...]) -> np.ndarray:\n",
    "    \"\"\"Sum x over broadcasted axes so that it matches 'shape'.\"\"\"\n",
    "    if x.shape == shape:\n",
    "        return x\n",
    "    # Remove leading broadcasted dims\n",
    "    while x.ndim > len(shape):\n",
    "        x = x.sum(axis=0)\n",
    "    # Sum over axes where original had size 1\n",
    "    for i, (sx, so) in enumerate(zip(x.shape, shape)):\n",
    "        if so == 1 and sx != 1:\n",
    "            x = x.sum(axis=i, keepdims=True)\n",
    "    return x.reshape(shape)\n",
    "\n",
    "def _expand_grad_to_shape(g: np.ndarray, in_shape: Tuple[int, ...],\n",
    "                          axis: Optional[Tuple[int, ...]], keepdims: bool) -> np.ndarray:\n",
    "    \"\"\"Expand reduced gradient g back to input shape for sum/mean backward.\"\"\"\n",
    "    if axis is None:\n",
    "        return np.broadcast_to(g, in_shape)\n",
    "    axes = axis\n",
    "    # Re-insert reduced dims if keepdims=False\n",
    "    if not keepdims:\n",
    "        # Insert dims in sorted order to keep positions right\n",
    "        for ax in axes:\n",
    "            g = np.expand_dims(g, ax)\n",
    "    return np.broadcast_to(g, in_shape)\n",
    "\n",
    "def _swap_last2(x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim < 2:\n",
    "        return x  # no-op for 0D/1D\n",
    "    return np.swapaxes(x, -1, -2)\n",
    "\n",
    "def _scatter_add_along_axis(base: np.ndarray, axis: int,\n",
    "                            indices: np.ndarray, updates: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Scatter-add 'updates' into 'base' along 'axis' using 'indices' like the inverse of take_along_axis.\n",
    "    Modifies 'base' in-place.\n",
    "    \"\"\"\n",
    "    axis = axis if axis >= 0 else axis + base.ndim\n",
    "    base_m = np.moveaxis(base, axis, 0)\n",
    "    idx_m = np.moveaxis(indices, axis, 0)\n",
    "    upd_m = np.moveaxis(updates, axis, 0)\n",
    "\n",
    "    # Build coordinates for advanced indexing\n",
    "    # idx_m shape: (K, s1, s2, ...); grid gives coordinates for remaining axes\n",
    "    grid = np.indices(idx_m.shape)\n",
    "    # grid[0] are positions along the axis of 'updates', which we don't need for base (replaced by idx_m)\n",
    "    coords = [idx_m] + [grid[i] for i in range(1, grid.shape[0])]\n",
    "    np.add.at(base_m, tuple(coords), upd_m)\n",
    "\n",
    "def _to_tensor(x: ArrayLike, requires_grad: bool = False) -> \"Tensor\":\n",
    "    return x if isinstance(x, Tensor) else Tensor(x, requires_grad=requires_grad)\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data: ArrayLike, requires_grad: bool = False, name: Optional[str] = None):\n",
    "        if isinstance(data, Tensor):\n",
    "            data = data.data\n",
    "        self.data: np.ndarray = _as_array(data)\n",
    "        self.grad: Optional[np.ndarray] = None\n",
    "        self.requires_grad: bool = requires_grad\n",
    "        self._backward = lambda: None\n",
    "        self._prev: set[Tensor] = set()\n",
    "        self.name = name  # optional debug label\n",
    "\n",
    "    # ----------- Utility -----------\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        return self.data.shape\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Recursively zero grads for all nodes reachable from this Tensor.\"\"\"\n",
    "        visited = set()\n",
    "        def visit(t: Tensor):\n",
    "            if t in visited:\n",
    "                return\n",
    "            visited.add(t)\n",
    "            t.grad = None\n",
    "            for p in t._prev:\n",
    "                visit(p)\n",
    "        visit(self)\n",
    "\n",
    "    def detach(self) -> \"Tensor\":\n",
    "        out = Tensor(self.data.copy(), requires_grad=False, name=f\"{self.name}_detached\" if self.name else None)\n",
    "        return out\n",
    "\n",
    "    def numpy(self) -> np.ndarray:\n",
    "        return self.data\n",
    "\n",
    "    def item(self) -> float:\n",
    "        return float(self.data.item())\n",
    "\n",
    "    # ----------- Autograd core -----------\n",
    "    def backward(self, grad: Optional[ArrayLike] = None):\n",
    "        if grad is None:\n",
    "            if self.data.size != 1:\n",
    "                raise ValueError(\"grad must be specified for non-scalar outputs\")\n",
    "            grad = np.ones_like(self.data, dtype=np.float32)\n",
    "        else:\n",
    "            grad = _as_array(grad)\n",
    "\n",
    "        topo: list[Tensor] = []\n",
    "        visited: set[Tensor] = set()\n",
    "\n",
    "        def build(v: Tensor):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "        self.grad = grad\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    # ----------- Overloads -----------\n",
    "    def __add__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        other = _to_tensor(other)\n",
    "        data = self.data + other.data\n",
    "        out = Tensor(data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                g = _sum_to_shape(out.grad, self.shape)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "            if other.requires_grad:\n",
    "                g = _sum_to_shape(out.grad, other.shape)\n",
    "                other.grad = g if other.grad is None else other.grad + g\n",
    "\n",
    "        out._prev = {self, other}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    __radd__ = __add__\n",
    "\n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        data = -self.data\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = -out.grad\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __sub__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        return self + (-_to_tensor(other))\n",
    "    def __rsub__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        return _to_tensor(other) + (-self)\n",
    "\n",
    "    def __mul__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        other = _to_tensor(other)\n",
    "        data = self.data * other.data\n",
    "        out = Tensor(data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                g = out.grad * other.data\n",
    "                g = _sum_to_shape(g, self.shape)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "            if other.requires_grad:\n",
    "                g = out.grad * self.data\n",
    "                g = _sum_to_shape(g, other.shape)\n",
    "                other.grad = g if other.grad is None else other.grad + g\n",
    "\n",
    "        out._prev = {self, other}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __rmul__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        other = _to_tensor(other)\n",
    "        data = self.data / other.data\n",
    "        out = Tensor(data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                g = out.grad / other.data\n",
    "                g = _sum_to_shape(g, self.shape)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "            if other.requires_grad:\n",
    "                g = -out.grad * self.data / (other.data ** 2)\n",
    "                g = _sum_to_shape(g, other.shape)\n",
    "                other.grad = g if other.grad is None else other.grad + g\n",
    "\n",
    "        out._prev = {self, other}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __rtruediv__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        other = _to_tensor(other)\n",
    "        return other / self\n",
    "\n",
    "    def __pow__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        # supports scalar or Tensor exponent\n",
    "        if isinstance(other, Tensor):\n",
    "            base = self\n",
    "            exp = other\n",
    "            data = np.power(base.data, exp.data)\n",
    "            out = Tensor(data, requires_grad=base.requires_grad or exp.requires_grad)\n",
    "\n",
    "            def _backward():\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                # d/dx x^y = x^y * y / x ; d/dy x^y = x^y * log(x)\n",
    "                safe_base = np.where(base.data == 0, 1.0, base.data)\n",
    "                if base.requires_grad:\n",
    "                    g = out.grad * data * (exp.data / safe_base)\n",
    "                    g = _sum_to_shape(g, base.shape)\n",
    "                    base.grad = g if base.grad is None else base.grad + g\n",
    "                if exp.requires_grad:\n",
    "                    # log defined only for base > 0; clip to avoid nan in toy engine\n",
    "                    g = out.grad * data * np.log(np.clip(base.data, 1e-12, None))\n",
    "                    g = _sum_to_shape(g, exp.shape)\n",
    "                    exp.grad = g if exp.grad is None else exp.grad + g\n",
    "\n",
    "            out._prev = {base, exp}\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            exp = float(other)\n",
    "            data = self.data ** exp\n",
    "            out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "            def _backward():\n",
    "                if out.grad is None or not self.requires_grad:\n",
    "                    return\n",
    "                g = out.grad * (exp * (self.data ** (exp - 1.0)))\n",
    "                g = _sum_to_shape(g, self.shape)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "            out._prev = {self}\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "    def __rpow__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        other = _to_tensor(other)\n",
    "        return other ** self\n",
    "\n",
    "    def __matmul__(self, other: ArrayLike) -> \"Tensor\":\n",
    "        other = _to_tensor(other)\n",
    "        data = np.matmul(self.data, other.data)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                g = np.matmul(out.grad, _swap_last2(other.data))\n",
    "                g = _sum_to_shape(g, self.shape)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "            if other.requires_grad:\n",
    "                g = np.matmul(_swap_last2(self.data), out.grad)\n",
    "                g = _sum_to_shape(g, other.shape)\n",
    "                other.grad = g if other.grad is None else other.grad + g\n",
    "\n",
    "        out._prev = {self, other}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ----------- Functions -----------\n",
    "    @property\n",
    "    def T(self) -> \"Tensor\":\n",
    "        data = _swap_last2(self.data)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = _swap_last2(out.grad)\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def reshape(self, *shape: int) -> \"Tensor\":\n",
    "        # Allow reshape((...)) or reshape(a,b,c)\n",
    "        new_shape = shape[0] if len(shape) == 1 and isinstance(shape[0], (tuple, list)) else shape\n",
    "        data = self.data.reshape(*new_shape)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "        old_shape = self.shape\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = out.grad.reshape(old_shape)\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> \"Tensor\":\n",
    "        nd = self.data.ndim\n",
    "        if end_dim < 0:\n",
    "            end_dim += nd\n",
    "        if not (0 <= start_dim <= end_dim < nd):\n",
    "            raise ValueError(\"Invalid flatten dims\")\n",
    "        pre = self.shape[:start_dim]\n",
    "        mid = int(np.prod(self.shape[start_dim:end_dim+1], dtype=np.int64))\n",
    "        post = self.shape[end_dim+1:]\n",
    "        return self.reshape(*(pre + (mid,) + post))\n",
    "\n",
    "    def exp(self) -> \"Tensor\":\n",
    "        data = np.exp(self.data)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = out.grad * out.data  # derivative of exp is exp(x)\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self) -> \"Tensor\":\n",
    "        data = np.log(self.data)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = out.grad / self.data\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def sqrt(self) -> \"Tensor\":\n",
    "        data = np.sqrt(self.data)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = out.grad * (0.5 / np.sqrt(self.data))\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def sum(self, axis: Optional[Union[int, Iterable[int]]] = None,\n",
    "            dim: Optional[Union[int, Iterable[int]]] = None,\n",
    "            keepdims: bool = False) -> \"Tensor\":\n",
    "        ax = dim if dim is not None else axis\n",
    "        ax_tuple = _normalize_axes(ax, self.data.ndim)\n",
    "        data = self.data.sum(axis=ax_tuple, keepdims=keepdims)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "        in_shape = self.shape\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = _expand_grad_to_shape(out.grad, in_shape, ax_tuple, keepdims)\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self, axis: Optional[Union[int, Iterable[int]]] = None,\n",
    "             dim: Optional[Union[int, Iterable[int]]] = None,\n",
    "             keepdims: bool = False) -> \"Tensor\":\n",
    "        ax = dim if dim is not None else axis\n",
    "        ax_tuple = _normalize_axes(ax, self.data.ndim)\n",
    "        data = self.data.mean(axis=ax_tuple, keepdims=keepdims)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "        in_shape = self.shape\n",
    "\n",
    "        # number of elements reduced\n",
    "        if ax_tuple is None:\n",
    "            N = self.data.size\n",
    "        else:\n",
    "            N = 1\n",
    "            for a in ax_tuple:\n",
    "                N *= self.data.shape[a]\n",
    "        N = float(N)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g = out.grad / N\n",
    "            g = _expand_grad_to_shape(g, in_shape, ax_tuple, keepdims)\n",
    "            self.grad = g if self.grad is None else self.grad + g\n",
    "\n",
    "        out._prev = {self}\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def gather(self, indices: ArrayLike,\n",
    "               axis: Optional[int] = None,\n",
    "               dim: Optional[int] = None) -> \"Tensor\":\n",
    "        \"\"\"\n",
    "        Gather values along an axis using integer indices (like torch.gather).\n",
    "        - indices: int array/Tensor. Output shape == indices.shape.\n",
    "        - axis/dim: axis to gather from (default 0). Supports negative axes.\n",
    "        Note: indices are treated as non-differentiable.\n",
    "        \"\"\"\n",
    "        ax = dim if dim is not None else (axis if axis is not None else 0)\n",
    "        idx = indices.data.astype(np.int64) if isinstance(indices, Tensor) else np.asarray(indices, dtype=np.int64)\n",
    "        data = np.take_along_axis(self.data, idx, axis=ax)\n",
    "        out = Tensor(data, requires_grad=self.requires_grad)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None or not self.requires_grad:\n",
    "                return\n",
    "            g_in = np.zeros_like(self.data, dtype=np.float32)\n",
    "            _scatter_add_along_axis(g_in, ax, idx, out.grad)\n",
    "            self.grad = g_in if self.grad is None else self.grad + g_in\n",
    "\n",
    "        out._prev = {self}  # indices are non-differentiable\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ----------- Pretty print -----------\n",
    "    def __repr__(self) -> str:\n",
    "        rg = \" req_grad\" if self.requires_grad else \"\"\n",
    "        return f\"Tensor(shape={self.data.shape},{rg}, dtype={self.data.dtype}{', name='+self.name if self.name else ''})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba87fda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad shape: (4, 3, 5)\n",
      "b.grad shape: (5,)\n",
      "(8, 10, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "grad must be specified for non-scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m w\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma.grad shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, a\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mshape)   \u001b[38;5;66;03m# (8,10,16)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw.grad shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, w\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mshape)   \u001b[38;5;66;03m# (8,16,32)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad must be specified for non-scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m     grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: grad must be specified for non-scalar outputs"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Broadcasting add/mul with batch\n",
    "    x = Tensor(np.random.randn(4, 3, 5), requires_grad=True, name=\"x\")\n",
    "    b = Tensor(np.random.randn(5), requires_grad=True, name=\"b\")\n",
    "    y = (x + b).mul = x + b\n",
    "    z = ((x * 2.0) + y).mean()\n",
    "    z.backward()\n",
    "    print(\"x.grad shape:\", x.grad.shape)   # (4,3,5)\n",
    "    print(\"b.grad shape:\", b.grad.shape)   # (5,)\n",
    "\n",
    "    # Batched matmul\n",
    "    a = Tensor(np.random.randn(8, 10, 16), requires_grad=True, name=\"a\")\n",
    "    w = Tensor(np.random.randn( 16, 32), requires_grad=True, name=\"w\")\n",
    "    out = a @ w\n",
    "    print(out.shape)\n",
    "    out.backward()\n",
    "    print(\"a.grad shape:\", a.grad.shape)   # (8,10,16)\n",
    "    print(\"w.grad shape:\", w.grad.shape)   # (8,16,32)\n",
    "\n",
    "    # Sum and mean over multiple axes\n",
    "    u = Tensor(np.random.randn(2, 3, 4), requires_grad=True, name=\"u\")\n",
    "    s = u.sum(axis=(0, 2))\n",
    "    s.backward(np.ones_like(s.data))\n",
    "    print(\"u.grad shape:\", u.grad.shape)   # (2,3,4)\n",
    "\n",
    "    u.zero_grad()\n",
    "    m = u.mean(dim=1, keepdims=True)\n",
    "    m.backward(np.ones_like(m.data))\n",
    "    print(\"u.grad shape (mean):\", u.grad.shape)  # (2,3,4)\n",
    "\n",
    "    # Gather along axis (like selecting token embeddings)\n",
    "    emb = Tensor(np.random.randn(4, 7, 16), requires_grad=True, name=\"emb\")  # (B, V, D)\n",
    "    idx = np.random.randint(0, 7, size=(4, 5, 16))  # gather along axis=1 -> output (4,5,16)\n",
    "    picked = emb.gather(idx, axis=1)\n",
    "    loss = picked.sum()\n",
    "    loss.backward()\n",
    "    print(\"emb.grad shape:\", emb.grad.shape)  # (4,7,16)\n",
    "\n",
    "    # Transpose last two dims\n",
    "    M = Tensor(np.random.randn(3, 5, 7), requires_grad=True, name=\"M\")\n",
    "    Mt = M.T\n",
    "    v = (M @ Mt).sum()\n",
    "    v.backward()\n",
    "    print(\"M.grad shape:\", M.grad.shape)  # (3,5,7)\n",
    "\n",
    "    # Reshape/flatten\n",
    "    t = Tensor(np.random.randn(2, 3, 4), requires_grad=True)\n",
    "    r = t.flatten(1, 2)  # -> (2, 12)\n",
    "    (r.sum()).backward()\n",
    "    print(\"t.grad shape:\", t.grad.shape)  # (2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d5d67",
   "metadata": {},
   "source": [
    "# Chatgpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74570c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd Tensor implementation for batch/matrix/3D data\n",
    "# - Supports: +, -, neg, *, **, sqrt, @, /, T, reshape, exp, log, mean, gather, sum\n",
    "# - sum and gather accept axis/dim arguments\n",
    "# - Backprop handles broadcasting and batched matmul\n",
    "# - Includes small tests demonstrating gradients on batch tensors\n",
    "import numpy as np\n",
    "\n",
    "# Utility helpers\n",
    "def ensure_numpy(x):\n",
    "    if isinstance(x, Tensor):\n",
    "        return x.data\n",
    "    elif isinstance(x, (np.ndarray, float, int)):\n",
    "        return np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "    else:\n",
    "        return np.array(x)\n",
    "\n",
    "def _unbroadcast(grad, shape):\n",
    "    # Sum out broadcasted dimensions so grad matches `shape`.\n",
    "    if grad.shape == shape:\n",
    "        return grad\n",
    "    # Sum trailing dimensions introduced by broadcasting\n",
    "    # Example: grad (2,3,4) -> shape (3,4) -> need to sum axis 0\n",
    "    while grad.ndim > len(shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "    # For same ndim, sum axes where shape==1 but grad>1\n",
    "    for i, (g_dim, s_dim) in enumerate(zip(grad.shape, shape)):\n",
    "        if s_dim == 1 and g_dim != 1:\n",
    "            grad = grad.sum(axis=i, keepdims=True)\n",
    "    return grad.reshape(shape)\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False, _children=(), _op=''):\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data, Tensor):\n",
    "            data = data.data.copy()\n",
    "        elif not isinstance(data, np.ndarray):\n",
    "            data = np.array(data)\n",
    "        self.data = data.astype(float)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # for graph viz/debug\n",
    "        self.shape = self.data.shape\n",
    "\n",
    "    # Basic properties\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.data.ndim\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.data.ndim\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        out = Tensor(self.data.T, requires_grad=self.requires_grad, _children=(self,), _op='T')\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                g = out.grad.T\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def reshape(self, *shape):\n",
    "        new_shape = shape if len(shape) > 1 else shape[0] if isinstance(shape[0], (tuple, list)) else shape\n",
    "        out = Tensor(self.data.reshape(new_shape), requires_grad=self.requires_grad, _children=(self,), _op='reshape')\n",
    "        old_shape = self.shape\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if out.grad is None:\n",
    "                    return\n",
    "                g = out.grad.reshape(old_shape)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Representation\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "    # Operator helpers for binary ops with broadcasting\n",
    "    def _binary_op(self, other, op_name, forward, grad_self, grad_other):\n",
    "        other_data = other.data if isinstance(other, Tensor) else np.array(other)\n",
    "        out_data = forward(self.data, other_data)\n",
    "        requires = self.requires_grad or (other.requires_grad if isinstance(other, Tensor) else False)\n",
    "        out = Tensor(out_data, requires_grad=requires, _children=(self, other if isinstance(other, Tensor) else ()), _op=op_name)\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                g_self = grad_self(out.grad, self.data, other_data)\n",
    "                g_self = _unbroadcast(g_self, self.shape)\n",
    "                self.grad = g_self if self.grad is None else self.grad + g_self\n",
    "            if isinstance(other, Tensor) and other.requires_grad:\n",
    "                g_other = grad_other(out.grad, self.data, other_data)\n",
    "                g_other = _unbroadcast(g_other, other.shape)\n",
    "                other.grad = g_other if other.grad is None else other.grad + g_other\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Addition\n",
    "    def __add__(self, other):\n",
    "        return self._binary_op(other, 'add', lambda a,b: a + b,\n",
    "                               lambda gout,a,b: gout,\n",
    "                               lambda gout,a,b: gout)\n",
    "    __radd__ = __add__\n",
    "\n",
    "    # Subtraction\n",
    "    def __sub__(self, other):\n",
    "        return self._binary_op(other, 'sub', lambda a,b: a - b,\n",
    "                               lambda gout,a,b: gout,\n",
    "                               lambda gout,a,b: -gout)\n",
    "    def __rsub__(self, other):\n",
    "        # other - self\n",
    "        other_t = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return other_t.__sub__(self)\n",
    "\n",
    "    # Negation\n",
    "    def __neg__(self):\n",
    "        out = Tensor(-self.data, requires_grad=self.requires_grad, _children=(self,), _op='neg')\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                g = -out.grad\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Multiplication\n",
    "    def __mul__(self, other):\n",
    "        return self._binary_op(other, 'mul', lambda a,b: a * b,\n",
    "                               lambda gout,a,b: gout * b,\n",
    "                               lambda gout,a,b: gout * a)\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    # Division\n",
    "    def __truediv__(self, other):\n",
    "        return self._binary_op(other, 'div', lambda a,b: a / b,\n",
    "                               lambda gout,a,b: gout / b,\n",
    "                               lambda gout,a,b: -gout * a / (b**2))\n",
    "    def __rtruediv__(self, other):\n",
    "        other_t = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return other_t.__truediv__(self)\n",
    "\n",
    "    # Power\n",
    "    def __pow__(self, power):\n",
    "        if isinstance(power, Tensor):\n",
    "            children = (self, power)\n",
    "        else:\n",
    "            children = (self,)\n",
    "        power_val = power.data if isinstance(power, Tensor) else power\n",
    "        out = Tensor(self.data ** power_val, requires_grad=self.requires_grad or (isinstance(power, Tensor) and power.requires_grad), _children=children, _op='pow')\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                g_self = out.grad * (power_val * (self.data ** (power_val - 1)))\n",
    "                g_self = _unbroadcast(g_self, self.shape)\n",
    "                self.grad = g_self if self.grad is None else self.grad + g_self\n",
    "            if isinstance(power, Tensor) and power.requires_grad:\n",
    "                # d/dp a^p = a^p * log(a)\n",
    "                g_p = out.grad * (self.data ** power_val) * np.log(self.data + 1e-20)\n",
    "                g_p = _unbroadcast(g_p, power.shape)\n",
    "                power.grad = g_p if power.grad is None else power.grad + g_p\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # sqrt\n",
    "    def sqrt(self):\n",
    "        return self.__pow__(0.5)\n",
    "\n",
    "    # exp, log\n",
    "    def exp(self):\n",
    "        out = Tensor(np.exp(self.data), requires_grad=self.requires_grad, _children=(self,), _op='exp')\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                g = out.grad * out.data  # exp(x) derivative is exp(x)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        out = Tensor(np.log(self.data + 1e-20), requires_grad=self.requires_grad, _children=(self,), _op='log')\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                g = out.grad / (self.data + 1e-20)\n",
    "                self.grad = g if self.grad is None else self.grad + g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Matrix multiplication (supports batched matmul via np.matmul)\n",
    "    def __matmul__(self, other):\n",
    "        other_data = other.data if isinstance(other, Tensor) else np.array(other)\n",
    "        out_data = self.data @ other_data\n",
    "        requires = self.requires_grad or (other.requires_grad if isinstance(other, Tensor) else False)\n",
    "        out = Tensor(out_data, requires_grad=requires, _children=(self, other if isinstance(other, Tensor) else ()), _op='matmul')\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            g = out.grad\n",
    "            if self.requires_grad:\n",
    "                # grad wrt a: g @ b.T (with proper broadcasting)\n",
    "                grad_a = g @ np.swapaxes(other_data, -1, -2)\n",
    "                grad_a = _unbroadcast(grad_a, self.shape)\n",
    "                self.grad = grad_a if self.grad is None else self.grad + grad_a\n",
    "            if isinstance(other, Tensor) and other.requires_grad:\n",
    "                grad_b = np.swapaxes(self.data, -1, -2) @ g\n",
    "                grad_b = _unbroadcast(grad_b, other.shape)\n",
    "                other.grad = grad_b if other.grad is None else other.grad + grad_b\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Sum with axis/dim support\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        out_data = self.data.sum(axis=axis, keepdims=keepdims)\n",
    "        out = Tensor(out_data, requires_grad=self.requires_grad, _children=(self,), _op='sum')\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                grad = out.grad\n",
    "                if not keepdims and axis is not None:\n",
    "                    # need to reshape grad to have singleton dims where sum occurred\n",
    "                    if isinstance(axis, int):\n",
    "                        axes = (axis,)\n",
    "                    else:\n",
    "                        axes = tuple(axis)\n",
    "                    shape = list(self.shape)\n",
    "                    for ax in axes:\n",
    "                        shape[ax] = 1\n",
    "                    grad = grad.reshape(shape)\n",
    "                # broadcast to self.shape\n",
    "                grad = np.broadcast_to(grad, self.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # mean over axis(s)\n",
    "    def mean(self, axis=None, keepdims=False):\n",
    "        if axis is None:\n",
    "            denom = self.data.size\n",
    "        else:\n",
    "            if isinstance(axis, int):\n",
    "                axes = (axis,)\n",
    "            else:\n",
    "                axes = tuple(axis)\n",
    "            denom = 1\n",
    "            for ax in axes:\n",
    "                denom *= self.shape[ax]\n",
    "        out_data = self.data.mean(axis=axis, keepdims=keepdims)\n",
    "        out = Tensor(out_data, requires_grad=self.requires_grad, _children=(self,), _op='mean')\n",
    "        def _backward():\n",
    "            if self.requires_grad and out.grad is not None:\n",
    "                grad = out.grad\n",
    "                if not keepdims and axis is not None:\n",
    "                    if isinstance(axis, int):\n",
    "                        axes = (axis,)\n",
    "                    else:\n",
    "                        axes = tuple(axis)\n",
    "                    shape = list(self.shape)\n",
    "                    for ax in axes:\n",
    "                        shape[ax] = 1\n",
    "                    grad = grad.reshape(shape)\n",
    "                grad = np.broadcast_to(grad, self.shape) / denom\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # gather with dim/axis\n",
    "    # index is expected to be integer Tensor with indices along `dim`\n",
    "    def gather(self, dim, index):\n",
    "        idx = index.data.astype(int) if isinstance(index, Tensor) else np.array(index).astype(int)\n",
    "        out_data = np.take_along_axis(self.data, idx, axis=dim)\n",
    "        requires = self.requires_grad or (index.requires_grad if isinstance(index, Tensor) else False)\n",
    "        children = (self, index) if isinstance(index, Tensor) else (self,)\n",
    "\n",
    "        out = Tensor(\n",
    "            out_data,\n",
    "            requires_grad=requires,\n",
    "            _children=children,\n",
    "            _op='gather'\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            if out.grad is None:\n",
    "                return\n",
    "            if self.requires_grad:\n",
    "                # scatter out.grad back into shape of self.data using idx\n",
    "                grad_self = np.zeros_like(self.data)\n",
    "                # Build full index arrays for np.add.at\n",
    "                # Create grid of indices for all axes\n",
    "                # idx has same shape as out.grad\n",
    "                # We want to index at positions where along axis dim we put idx, and other axes are broadcasted from np.indices\n",
    "                grid = np.ogrid[tuple(slice(0, s) for s in out.grad.shape)]\n",
    "                # Convert to list of index arrays for advanced indexing\n",
    "                indexer = []\n",
    "                for axis in range(self.data.ndim):\n",
    "                    if axis == dim:\n",
    "                        # need idx entries\n",
    "                        indexer.append(idx)\n",
    "                    else:\n",
    "                        # expand index grid for this axis to shape of idx\n",
    "                        # find corresponding axis in out.grad: mapping depends on shapes; assume shapes match except along dim\n",
    "                        # We'll create an index array by broadcasting arange of that axis length to out.grad shape\n",
    "                        length = self.data.shape[axis]\n",
    "                        shape = [1]*out.grad.ndim\n",
    "                        # Determine where this axis falls in out.grad shape:\n",
    "                        # We assume out.grad shape equals idx.shape\n",
    "                        # We need to get an array of indices 0..length-1 placed into the proper axis location so it broadcasts\n",
    "                        arr = np.arange(length).reshape([-1 if i==0 else 1 for i in range(1)])  # placeholder (won't use)\n",
    "                        # A simpler approach: construct indexer by using np.indices of out.grad.shape\n",
    "                        pass\n",
    "                # Simpler scatter using loop over flattened entries (vectorized enough for reasonable sizes)\n",
    "                flat_idx = idx.ravel()\n",
    "                flat_grad = out.grad.ravel()\n",
    "                # Compute multi-index for each element in flat_idx\n",
    "                # To find the coordinates in self.data corresponding to each element in out.grad, we need coordinates for other axes.\n",
    "                # Use np.indices to get coordinate grids for out.grad's shape\n",
    "                coords = np.indices(out.grad.shape)\n",
    "                # coords is shape (ndim_out, *out.grad.shape). For each axis of out.grad, coords[ax] gives indices along that axis.\n",
    "                # We must map coords axes to self.data axes. When self.data and out.grad shapes differ only on dim,\n",
    "                # assume other axes match sizes. So coords along axis j correspond to same axis j in self.data except at dim where we use flat_idx.\n",
    "                # Build tuple of arrays for destination indices:\n",
    "                dest_idx = []\n",
    "                for axis in range(self.data.ndim):\n",
    "                    if axis == dim:\n",
    "                        dest_idx.append(flat_idx.reshape(out.grad.shape))\n",
    "                    else:\n",
    "                        dest_idx.append(coords[axis])\n",
    "                # Use numpy.add.at with tuple(dest_idx)\n",
    "                np.add.at(grad_self, tuple(dest_idx), out.grad)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "            # Note: index (indices) is usually integer, not differentiable; we skip gradient for index.\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # Sum alias\n",
    "    def sum_all(self):\n",
    "        return self.sum(axis=None)\n",
    "\n",
    "    # backward engine\n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None:\n",
    "            grad = np.ones_like(self.data)\n",
    "        self.grad = grad if self.grad is None else self.grad + grad\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    if not isinstance(child, Tensor):\n",
    "                        raise TypeError(f\"Graph child is not a Tensor, got {type(child)}\")\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f199686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[2. 3.]\n",
      " [4. 5.]], requires_grad=True)\n",
      "Tensor(data=[[ 5.41421356 10.73205081]\n",
      " [18.         27.23606798]], requires_grad=True)\n",
      "\n",
      "Test4: math ops\n",
      "t.grad:\n",
      " [[ 4.35355339  6.28867513]\n",
      " [ 8.25       10.2236068 ]]\n",
      "\n",
      "Test5: gather\n",
      "g.data:\n",
      " [[30. 10.]\n",
      " [50. 60.]]\n",
      "src.grad:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4) power, exp, log, sqrt\n",
    "t = Tensor(np.array([[2.,3.],[4.,5.]]), requires_grad=True)\n",
    "z = (t ** 2).exp().log() + t.sqrt()\n",
    "res = z.sum()\n",
    "print(t)\n",
    "print(z)\n",
    "res.backward()\n",
    "print(\"\\nTest4: math ops\")\n",
    "print(\"t.grad:\\n\", t.grad)\n",
    "\n",
    "# Note: gather test (basic)\n",
    "src = Tensor(np.array([[10,20,30],[40,50,60]]).astype(float), requires_grad=True)\n",
    "idx = Tensor(np.array([[2,0],[1,2]]), requires_grad=False)\n",
    "g = src.gather(1, idx)  # gather along axis 1\n",
    "L = g.sum()\n",
    "L.backward()\n",
    "print(\"\\nTest5: gather\")\n",
    "print(\"g.data:\\n\", g.data)\n",
    "print(\"src.grad:\\n\", src.grad)\n",
    "\n",
    "# End of demonstration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b86e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([[1, 2], [3, 4]]), requires_grad=True)\n",
    "b = Tensor(np.random.randint(0,4,size = (2,3,2)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fdd92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b@a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ee95a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7c734a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       " \n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]]),\n",
       " array([[[3., 7.],\n",
       "         [3., 7.],\n",
       "         [3., 7.]],\n",
       " \n",
       "        [[3., 7.],\n",
       "         [3., 7.],\n",
       "         [3., 7.]]]),\n",
       " array([[ 9.,  9.],\n",
       "        [11., 11.]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad, b.grad, a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0ed891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(np.random.randint(0,10,size= (2,3,3)), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ebce9dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[[2. 8. 4.]\n",
       "  [2. 3. 8.]\n",
       "  [0. 4. 3.]]\n",
       "\n",
       " [[2. 0. 9.]\n",
       "  [8. 2. 7.]\n",
       "  [8. 0. 2.]]], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0667b907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3, 3),\n",
       " Tensor(data=[[ 4. 15. 15.]\n",
       "  [18.  2. 18.]], requires_grad=True))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3b3225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Tensor([1,2,3,4,5,6], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "314e3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.gather(dim = 0, index = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc2b4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = z.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cfa2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5791b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
